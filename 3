import numpy as np
import random

# -------------------------
# Maze Environment
# -------------------------
class Maze:
    def __init__(self):
        self.rows = 5
        self.cols = 5
        self.start = (0, 0)
        self.goal = (4, 4)
        self.traps = [(1, 3), (3, 1)]
        self.actions = ['up', 'down', 'left', 'right']

    def step(self, state, action):
        x, y = state

        if action == 'up':
            x -= 1
        elif action == 'down':
            x += 1
        elif action == 'left':
            y -= 1
        elif action == 'right':
            y += 1

        # Boundary check
        x = max(0, min(self.rows - 1, x))
        y = max(0, min(self.cols - 1, y))

        next_state = (x, y)

        if next_state == self.goal:
            return next_state, 10, True
        elif next_state in self.traps:
            return next_state, -10, True
        else:
            return next_state, -1, False


# -------------------------
# TD(0) Algorithm
# -------------------------
def td_zero(episodes=1000, alpha=0.1, gamma=0.9):
    env = Maze()
    V = np.zeros((env.rows, env.cols))

    for episode in range(episodes):
        state = env.start
        done = False

        while not done:
            action = random.choice(env.actions)
            next_state, reward, done = env.step(state, action)

            x, y = state
            nx, ny = next_state

            # TD(0) Update
            V[x, y] += alpha * (reward + gamma * V[nx, ny] - V[x, y])

            state = next_state

    return V


# -------------------------
# Run TD(0)
# -------------------------
value_function = td_zero()

print("State Value Function:")
print(np.round(value_function, 2))
